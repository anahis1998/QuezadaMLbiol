rf_pred
sum(m_rf$predicted==rf_pred)
length(rf_pred) #ok so predict is just pulling a slot that already is in m_rf
sum(rf_pred!=d_val$num)/dim(d_val)[1] #agrees with the oob error rate above.
gc()
#fit the models on all of the data excluding one group
rfres_s<-randomForest(num~.,
data=d_val[gp!=counter,],ntree=1000)
#get predictions for the left out group and get error rates
pred_rf_s<-predict(rfres_s,d_val[gp==counter,],type="class")
xerrs_rf[counter]<-sum(pred_rf_s!=d_val$num[gp==counter])/sum(gp==counter)
xerrs_rf<-NA*numeric(numgp)
xerrs_rf[counter]<-sum(pred_rf_s!=d_val$num[gp==counter])/sum(gp==counter)
mean(xerrs_rf)
xerrs_rf[counter]
xerrs_b[counter]
pred_rf_s!=d_val$num[gp==counter]
sum(gp==counter)
sum(pred_rf_s!=d_val$num[gp==counter])/sum(gp==counter)
sum(pred_5!=d_val$num[gp==counter])/sum(gp==counter)
xerrs_5[counter]
sum(pred_rf_s!=d_val$num[gp==counter])/sum(gp==counter)
sum(pred_b_s!=d_val$num[gp==counter])/sum(gp==counter)
m_ada <-boosting(num~.,
data = d_val,
mfinal = 10,
control = rpart.control(cp = 0, minsplit = 1, xval = 0))
ada_pred<-predict(m_ada,d_val[,2:dim(d_val)[2]])$class #note the prediction output gives
? predict
colnames(d_val)
d_val$age
ada_pred<-predict(m_ada,d_val[,2:dim(d_val)[2]])$class #note the prediction output gives
ada_pred <- predict(m_ada, newdata = d_val)$class
gc()
sum(ada_pred!=d_val$num)/dim(d_val)[1] #perfect, in sample
m_ada$importance #gives the relative importance of the different variables for
xerrs_ada<-NA*numeric(numgp)
xerrs_ada<-NA*numeric(numgp)
for (counter in 1:numgp)
{
print(paste(counter,"of",numgp))
#fit the models on all of the data excluding one group
adares_s<-boosting(num~.,data=d_val[gp!=counter,])
#get predictions for the left out group and get error rates
pred_ada_s<-predict(adares_s,d_val[gp==counter,],type="class")$class
xerrs_ada[counter]<-sum(pred_ada_s!=d_val$num[gp==counter])/sum(gp==counter)
}
mean(xerrs_ada) #best so far
#put the data in the specific form expected by the xgb function
X_val<-as.matrix(d_val[,2:(dim(d_val)[2])])
y_val<-as.integer(d_val[,1])-1
gc()
m_xgb<-xgboost(data=X_val,
label=y_val,
max_depth=6,eta=.3,
nthread=2,nrounds=20,
objective="binary:logistic",verbose=2)
sapply(d_val, class)
d_val[] <- lapply(d_val, function(x) {
if (is.character(x)) as.numeric(x) else x
})
X_val <- as.matrix(d_val[, 2:(dim(d_val)[2])])  # Usa todas las columnas excepto 'num'
y_val <- as.integer(d_val[, 1]) - 1              # Asegúrate de que 'num' sea el índice correcto
class(X_val)
m_xgb<-xgboost(data=X_val,
label=y_val,
max_depth=6,eta=.3,
nthread=2,nrounds=20,
objective="binary:logistic",verbose=2)
sapply(d_val, class)
d_val$ca <- as.numeric(as.character(d_val$ca))
d_val$thal <- as.numeric(as.character(d_val$thal))
X_val <- as.matrix(d_val[, 2:(dim(d_val)[2])])  # Asegúrate de que ahora todo sea numérico
y_val <- as.integer(d_val[, 1]) - 1              # Asegúrate de que 'num' sea el índice correcto
sapply(X_val, class)  # Debería mostrar "numeric" para todas las columnas
m_xgb<-xgboost(data=X_val,
label=y_val,
max_depth=6,eta=.3,
nthread=2,nrounds=20,
objective="binary:logistic",verbose=2)
sapply(d_val, class)
d_val[] <- lapply(d_val, function(x) {
if (is.factor(x)) {
as.numeric(as.character(x))  # Convierte factores a numéricos
} else if (is.character(x)) {
as.numeric(x)  # Convierte caracteres a numéricos
} else {
x  # Mantiene el tipo original si no es factor ni carácter
}
})
X_val <- as.matrix(d_val[, 2:(dim(d_val)[2])])  # Asegúrate de que ahora todo sea numérico
y_val <- as.integer(d_val[, 1]) - 1              # Asegúrate de que 'num' sea el índice correcto
sapply(X_val, class)  # Debería mostrar "numeric" para todas las columnas
m_xgb<-xgboost(data=X_val,
label=y_val,
max_depth=6,eta=.3,
nthread=2,nrounds=20,
objective="binary:logistic",verbose=2)
y_val <- ifelse(d_val$num >= 2, 1, 0)  # 1 para "enfermo", 0 para "sano"
m_xgb<-xgboost(data=X_val,
label=y_val,
max_depth=6,eta=.3,
nthread=2,nrounds=20,
objective="binary:logistic",verbose=2)
unique(y_val)  # Verifica los valores únicos en y_val
#get error rate on the validation data
pred_xgb<-predict(m_xgb,X_val) #gives probabilities
pred_xgb
#of being in the first class, which is additionally useful, tho
#we don't use it
predictions<-rep(0,length(pred_xgb))
predictions[pred_xgb>.5]<-"1"
sum(predictions!=as.character(d_val$num))/dim(d_val)[1]
xerrs_xgb<-NA*numeric(numgp)
for (counter in 1:numgp)
{
#status
print(paste(counter,"of",numgp))
#fit the models on all of the data excluding one group
m_xgb_s<-xgboost(data=X_val[gp!=counter,],
label=y_val[gp!=counter],
max_depth=7,eta=.2,subsample=0.5,nrounds=55,
nthread=2,objective="binary:logistic",verbose=0)
#get predictions for the left out group and get error rates
pred_xgb_s<-predict(m_xgb_s,X_val[gp==counter,])
predictions_s<-rep(0,length(pred_xgb_s))
predictions_s[pred_xgb_s>.5]<-"1"
xerrs_xgb[counter]<-sum(predictions_s!=
d_val$diagnosis[gp==counter])/sum(gp==counter)
}
mean(xerrs_xgb)
importance_matrix<-xgb.importance(model=m_xgb)
importance_matrix #You can get information on the importance of
d_test
testpred_ada<-predict(m_f_5,d_test[,2:14],type="class")$class
testpred_ada<-predict(m_f_5,d_test[,2:14],type="class")#$class
#So actually the predictions on the test data were slightly better than on
#x-val error rate, which is lucky (often it's a bit worse). But at the end
#of the day we have a very good classifier!
d_test$age
testpred_ada<-predict(m_f_5,d_test[,1:14],type="class")#$class
sum(testpred_ada!=d_test$num)/dim(d_test)[1]
mean(xerrs_rf)# --> 0.3181818
xerrs_f
mean(xerrs_f)#--> 0.4442688
mean(xerrs_d)#--> 0.4357708
mean(xerrs_5)#--> 0.2272727 *best value
mean(xerrs_b)#--> 0.3819999
mean(xerrs_rf)# --> 0.3181818
mean(xerrs_ada) #--> 0.391502
testpred_ada<-predict(m_ada,d_test[,1:14],type="class")#$class
sum(testpred_ada!=d_test$num)/dim(d_test)[1]
testpred_ada<-predict(m_ada,d_test[,1:14],type="class")$class
sum(testpred_ada!=d_test$num)/dim(d_test)[1]
testpred_ada<-predict(m_xgb,d_test[,1:14],type="class")$class
testpred_ada<-predict(m_xgb,d_test[,1:14],type="class")#$class
mean(xerrs_f)#--> 0.4442688
mean(xerrs_d)#--> 0.4357708
mean(xerrs_5)#--> 0.2272727 *best value
mean(xerrs_b)#--> 0.3819999
mean(xerrs_rf)# --> 0.3181818
mean(xerrs_ada) #--> 0.391502
mean(xerrs_xgb)#--> 0
testpred_ada<-predict(m_f,d_test[,1:14],type="class")#$class
sum(testpred_ada!=d_test$num)/dim(d_test)[1]
testpred_ada<-predict(m_d,d_test[,1:14],type="class")#$class
sum(testpred_ada!=d_test$num)/dim(d_test)[1]
#full cart
testpred_ada<-predict(m_f,d_test[,1:14],type="class")#$class
sum(testpred_ada!=d_test$num)/dim(d_test)[1] #--> 0.5263158
#full cart
testpred_ada<-predict(m_f_s,d_test[,1:14],type="class")#$class
sum(testpred_ada!=d_test$num)/dim(d_test)[1] #--> 0.5263158
#full cart
testpred_ada<-predict(m_d_s,d_test[,1:14],type="class")#$class
sum(testpred_ada!=d_test$num)/dim(d_test)[1] #--> 0.5263158
#bagging
testpred_ada<-predict(bagres,d_test[,1:14],type="class")#$class
sum(testpred_ada!=d_test$num)/dim(d_test)[1] #--> 0.5263158
#full cart
testpred_ada<-predict(m_d_s,d_test[,1:14],type="class")#$class
sum(testpred_ada!=d_test$num)/dim(d_test)[1] #--> 0.5263158
bagres
xerrs_5
#3.3 ------------Pruned CART
plotcp(m_f)
#3.3 ------------Pruned CART
plotcp(m_f)
printcp(m_f) #after checking the graf, the value is 0.017, BEST CP
plotcp(m_d)
#3.3 ------------Pruned CART
plotcp(m_f)
plotcp(m_d)
#3.3 ------------Pruned CART
plotcp(m_f)
plotcp(m_d)
#3.3 ------------Pruned CART
plotcp(m_f)
gc()
#my best cp --> cp coming from Full Cart=0.017
m_f_5<-rpart(num~.,
data=d_val,method="class",control=rpart.control(cp=0.045,minsplit=1))
m_f_5
m_f
#plots
plot(m_f_5,uniform=TRUE,margin=0.1)
text(m_f_5,use.n=TRUE,all=TRUE,cex=0.8)
x11()
plotcp(m_f_5)
plotcp(m_f)
xerrs_5<-NA*numeric(numgp)
for (counter in 1:numgp)
{
#fit the models on all of the data excluding one group
m_5_s<-rpart(num~.,
data=d_val[gp!=counter,],method="class",control=rpart.control(cp=0.017,minsplit=1))
#get predictions for the left-out group and get error rates
pred_5<-predict(m_5_s,d_val[gp==counter,],type="class")
xerrs_5[counter]<-sum(pred_5!=d_val$num[gp==counter])/sum(gp==counter)
}
#Time to check the means
mean(xerrs_f) #--> 0.4442688
mean(xerrs_d) #--> 0.4357708
mean(xerrs_5) #--> 0.2272727 So this new model beats both m_d and m_f
#fit the models on all of the data excluding one group
m_5_s<-rpart(num~.,
data=d_val[gp!=counter,],method="class",control=rpart.control(cp=0.045,minsplit=1))
#get predictions for the left-out group and get error rates
pred_5<-predict(m_5_s,d_val[gp==counter,],type="class")
xerrs_5[counter]<-sum(pred_5!=d_val$num[gp==counter])/sum(gp==counter)
{
#fit the models on all of the data excluding one group
m_5_s<-rpart(num~.,
data=d_val[gp!=counter,],method="class",control=rpart.control(cp=0.045,minsplit=1))
#get predictions for the left-out group and get error rates
pred_5<-predict(m_5_s,d_val[gp==counter,],type="class")
xerrs_5[counter]<-sum(pred_5!=d_val$num[gp==counter])/sum(gp==counter)
}
#Time to check the means
mean(xerrs_f) #--> 0.4442688
mean(xerrs_d) #--> 0.4357708
mean(xerrs_5) #--> 0.4043478 So this new model beats both m_d and m_f
#**Now apply THAT ONE MODEL ONLY on the test data and see prediction error
#--Pruned CART
testpred_ada<-predict(m_f_5,d_test[,1:14],type="class")#$class
unique(d_test$thal)
unique(d_test$ca)
#**Now apply THAT ONE MODEL ONLY on the test data and see prediction error
#--Pruned CART
testpred_ada<-predict(m_f_5,d_test[,1:14],type="class")$class
sum(testpred_ada!=d_test$num)/dim(d_test)[1] #--> 0.5526316
#**Now apply THAT ONE MODEL ONLY on the test data and see prediction error
#--Pruned CART
levels(d_train$ca)
#**Now apply THAT ONE MODEL ONLY on the test data and see prediction error
#--Pruned CART
levels(d_val$ca)
levels(d_test$ca)
#**Now apply THAT ONE MODEL ONLY on the test data and see prediction error
#--Pruned CART
d_train$ca <- as.factor(d_val$ca)
#**Now apply THAT ONE MODEL ONLY on the test data and see prediction error
#--Pruned CART
d_val$ca <- as.factor(d_val$ca)
d_val$thal <- as.factor(d_val$thal)
d_test$ca <- as.factor(d_test$ca)
d_test$thal <- as.factor(d_test$thal)
levels(d_val$ca)
levels(d_test$ca)
levels(d_val$thal)
levels(d_test$thal)
testpred_ada<-predict(m_f_5,d_test[,1:14],type="class")$class
testpred_ada<-predict(m_f_5,d_test[,1:14],type="class")#$class
levels(d_test$ca) <- gsub("\\.0", "", levels(d_test$ca))
levels(d_test$thal) <- gsub("\\.0", "", levels(d_test$thal))
d_val$ca <- as.factor(d_val$ca)
d_val$thal <- as.factor(d_val$thal)
d_test$ca <- as.factor(d_test$ca)
d_test$thal <- as.factor(d_test$thal)
levels(d_test$ca) <- levels(d_val$ca)
levels(d_test$thal) <- levels(d_val$thal)
testpred_ada<-predict(m_f_5,d_test[,1:14],type="class")#$class
testpred_ada<-predict(m_f_5,d_test[,1:14],type="class")$class
unique(d_val$ca)
unique(d_val$thal)
d_val
testpred_ada<-predict(m_f_5,d_test[,14],type="class")$class
testpred_ada <- predict(m_f_5, d_test[, -c(12, 13)], type = "class")$class
unique(d_test$thal)
unique(d_test$ca)
unique(d_val$ca)
unique(d_val$thal)
testpred_ada<-predict(m_f_5,d_test[,1:14],type="class")$class
testpred_ada<-predict(m_f_5,d_test[,1:14],type="class")#$class
#3.3 ------------Pruned CART
plotcp(m_f)
printcp(m_f) #after checking the graf, the value is 0.045, BEST CP
plotcp(m_d)
printcp(m_d) #after cheching the graf, the value is 0.044
pdf(file="ComplexityParameter_pre_pruned.pdf")
plotcp(m_d)
dev.off()
#my best cp --> cp coming from Full Cart=0.045
m_f_5<-rpart(num~.,
data=d_val,method="class",control=rpart.control(cp=0.045,minsplit=1))
m_f_5
m_f
#plots
plot(m_f_5,uniform=TRUE,margin=0.1)
text(m_f_5,use.n=TRUE,all=TRUE,cex=0.8)
xerrs_5<-NA*numeric(numgp)
for (counter in 1:numgp)
{
#fit the models on all of the data excluding one group
m_5_s<-rpart(num~.,
data=d_val[gp!=counter,],method="class",control=rpart.control(cp=0.045,minsplit=1))
#get predictions for the left-out group and get error rates
pred_5<-predict(m_5_s,d_val[gp==counter,],type="class")
xerrs_5[counter]<-sum(pred_5!=d_val$num[gp==counter])/sum(gp==counter)
}
#Time to check the means
mean(xerrs_f) #--> 0.4442688
mean(xerrs_d) #--> 0.4357708
mean(xerrs_5) #--> 0.3998024 So this new model beats both m_d and m_f
testpred_ada<-predict(m_f_5,d_test[,1:14],type="class")#$class
sum(testpred_ada!=d_test$num)/dim(d_test)[1] #--> 0.5526316
#Pre-pruned model
testpred_ada<-predict(m_d,d_test[,1:14],type="class")#$class
sum(testpred_ada!=d_test$num)/dim(d_test)[1] #--> 0.5263158
#full cart
testpred_ada<-predict(m_d_s,d_test[,1:14],type="class")#$class
sum(testpred_ada!=d_test$num)/dim(d_test)[1] #--> 0.5
#bagging
testpred_ada<-predict(bagres,d_test[,1:14],type="class")#$class
sum(testpred_ada!=d_test$num)/dim(d_test)[1] #--> 0.5263158
bagres <- bagging(num ~ .,
data = d_val,
nbagg = 500,
control = rpart.control(cp = 0, minsplit = 1, xval = 0),
aggregation = "majority")
bagres <- bagging(num ~ .,
data = d_val,
nbagg = 500,
control = rpart.control(cp = 0, minsplit = 1, xval = 0),
aggregation = "majority")
predictions <- predict(bagres, newdata = d_val, type="class")
m_rf<-randomForest(num~.,data=d_val,ntree=1000)
m_rf #includes oob error rate--> 37.44%
names(m_rf)
tail(m_rf$err.rate) #last entry of column 1 is the same as the oob error rate above
plot(m_rf) #You can also see as the number of trees increases the accuracy increases and then
#levels off
dim(m_rf$err.rate)[1] #corresponds to the number of trees=1000 trees
rf_pred<-predict(m_rf)
sum(m_rf$predicted==rf_pred)
length(rf_pred) #ok so predict is just pulling a slot that already is in m_rf
sum(rf_pred!=d_val$num)/dim(d_val)[1] #agrees with the oob error rate above --> 0.3744493
xerrs_rf<-NA*numeric(numgp)
for (counter in 1:numgp)
{
#fit the models on all of the data excluding one group
rfres_s<-randomForest(num~.,
data=d_val[gp!=counter,],ntree=1000)
#get predictions for the left out group and get error rates
pred_rf_s<-predict(rfres_s,d_val[gp==counter,],type="class")
xerrs_rf[counter]<-sum(pred_rf_s!=d_val$num[gp==counter])/sum(gp==counter)
}
mean(xerrs_f)#--> 0.4442688
mean(xerrs_d)#--> 0.4357708
mean(xerrs_5)#--> 0.2272727 *best value
mean(xerrs_b)#--> 0.3819999
mean(xerrs_rf)# --> 0.3181818
m_ada <-boosting(num~.,
data = d_val,
mfinal = 10,
control = rpart.control(cp = 0, minsplit = 1, xval = 0))
d_val$age
ada_pred <- predict(m_ada, newdata = d_val)$class
sum(ada_pred!=d_val$num)/dim(d_val)[1]
m_ada$importance #gives the relative importance of the different variables for
xerrs_ada<-NA*numeric(numgp)
for (counter in 1:numgp)
{
print(paste(counter,"of",numgp))
#fit the models on all of the data excluding one group
adares_s<-boosting(num~.,data=d_val[gp!=counter,])
#get predictions for the left out group and get error rates
pred_ada_s<-predict(adares_s,d_val[gp==counter,],type="class")$class
xerrs_ada[counter]<-sum(pred_ada_s!=d_val$num[gp==counter])/sum(gp==counter)
}
mean(xerrs_f)#--> 0.4442688
mean(xerrs_d)#--> 0.4357708
mean(xerrs_5)#--> 0.2272727 *best value
mean(xerrs_b)#--> 0.3819999
mean(xerrs_rf)# --> 0.3181818
mean(xerrs_ada) #--> 0.391502
m_xgb<-xgboost(data=X_val,
label=y_val,
max_depth=6,eta=.3,
nthread=2,nrounds=20,
objective="binary:logistic",verbose=2)
#get error rate on the validation data
pred_xgb<-predict(m_xgb,X_val) #gives probabilities
#of being in the first class, which is additionally useful, tho
#we don't use it
predictions<-rep(0,length(pred_xgb))
predictions[pred_xgb>.5]<-"1"
sum(predictions!=as.character(d_val$num))/dim(d_val)[1] #--> 0.4229075
xerrs_xgb<-NA*numeric(numgp)
for (counter in 1:numgp)
{
#status
print(paste(counter,"of",numgp))
#fit the models on all of the data excluding one group
m_xgb_s<-xgboost(data=X_val[gp!=counter,],
label=y_val[gp!=counter],
max_depth=7,eta=.2,subsample=0.5,nrounds=55,
nthread=2,objective="binary:logistic",verbose=0)
#get predictions for the left out group and get error rates
pred_xgb_s<-predict(m_xgb_s,X_val[gp==counter,])
predictions_s<-rep(0,length(pred_xgb_s))
predictions_s[pred_xgb_s>.5]<-"1"
xerrs_xgb[counter]<-sum(predictions_s!=
d_val$diagnosis[gp==counter])/sum(gp==counter)
}
mean(xerrs_f)#--> 0.4442688
mean(xerrs_d)#--> 0.4357708
mean(xerrs_5)#--> 0.2272727 *best value
mean(xerrs_b)#--> 0.3819999
mean(xerrs_rf)# --> 0.3181818
mean(xerrs_ada) #--> 0.391502
mean(xerrs_xgb)#--> 0
#**Now apply THAT ONE MODEL ONLY on the test data and see prediction error
#--Pruned CART
testpred_ada<-predict(m_f_5,d_test[,1:14],type="class")#$class
sum(testpred_ada!=d_test$num)/dim(d_test)[1] #--> 0.5526316
#Pre-pruned model
testpred_ada<-predict(m_d,d_test[,1:14],type="class")#$class
sum(testpred_ada!=d_test$num)/dim(d_test)[1] #--> 0.5263158
#full cart
testpred_ada<-predict(m_d_s,d_test[,1:14],type="class")#$class
sum(testpred_ada!=d_test$num)/dim(d_test)[1] #--> 0.5
#bagging
testpred_ada<-predict(bagres,d_test[,1:14],type="class")#$class
sum(testpred_ada!=d_test$num)/dim(d_test)[1] #--> 0.5263158
mean(xerrs_ada) #--> 1
#my best cp --> cp coming from Full Cart=0.045
m_f_5<-rpart(num~.,
data=d_val,method="class",control=rpart.control(cp=0.017,minsplit=1))
m_f_5
m_f
#plots
plot(m_f_5,uniform=TRUE,margin=0.1)
text(m_f_5,use.n=TRUE,all=TRUE,cex=0.8)
xerrs_5<-NA*numeric(numgp)
for (counter in 1:numgp)
{
#fit the models on all of the data excluding one group
m_5_s<-rpart(num~.,
data=d_val[gp!=counter,],method="class",control=rpart.control(cp=0.017,minsplit=1))
#get predictions for the left-out group and get error rates
pred_5<-predict(m_5_s,d_val[gp==counter,],type="class")
xerrs_5[counter]<-sum(pred_5!=d_val$num[gp==counter])/sum(gp==counter)
}
#Time to check the means
mean(xerrs_f) #--> 0.4442688
mean(xerrs_d) #--> 0.4357708
mean(xerrs_5) #--> 0.3513834 So this new model beats both m_d and m_f
xerrs_5
mean(xerrs_5) #--> 0.3513834 So this new model beats both m_d and m_f
plotcp(m_f)
plotcp(m_f_5)
#my best cp --> cp coming from Full Cart=0.045
m_f_5<-rpart(num~.,
data=d_val,method="class",control=rpart.control(cp=0.027,minsplit=1))
m_f_5
m_f
#plots
plot(m_f_5,uniform=TRUE,margin=0.1)
text(m_f_5,use.n=TRUE,all=TRUE,cex=0.8)
x11()
plotcp(m_f_5)
plotcp(m_f)
xerrs_5<-NA*numeric(numgp)
for (counter in 1:numgp)
{
#fit the models on all of the data excluding one group
m_5_s<-rpart(num~.,
data=d_val[gp!=counter,],method="class",control=rpart.control(cp=0.027,minsplit=1))
#get predictions for the left-out group and get error rates
pred_5<-predict(m_5_s,d_val[gp==counter,],type="class")
xerrs_5[counter]<-sum(pred_5!=d_val$num[gp==counter])/sum(gp==counter)
}
#Time to check the means
mean(xerrs_f) #--> 0.4442688
mean(xerrs_d) #--> 0.4357708
mean(xerrs_5) #--> 0.3513834 So this new model beats both m_d and m_f
mean(xerrs_b)#--> 0.3819999
mean(xerrs_5)#--> 0.3513834 *best value
mean(xerrs_b)#--> 0.3819999
mean(xerrs_rf)# --> 0.3181818
mean(xerrs_5)#--> 0.3515834 *best value
mean(xerrs_b)#--> 0.3819999
mean(xerrs_rf)# --> 0.3781818
mean(xerrs_ada) #--> 0.391502
mean(xerrs_f)#--> 0.4442688
mean(xerrs_d)#--> 0.4357708
mean(xerrs_5)#--> 0.3513834 *best value
mean(xerrs_xgb)#--> 0
#RESULTS
#
unique(d_val$num)
