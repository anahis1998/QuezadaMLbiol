plotcp(m_f_5)
plotcp(m_f)
xerrs_5<-NA*numeric(numgp)
for (counter in 1:numgp)
{
#fit the models on all of the data excluding one group
m_5_s<-rpart(num~.,
data=d_val[gp!=counter,],method="class",control=rpart.control(cp=0.027,minsplit=1))
#get predictions for the left-out group and get error rates
pred_5<-predict(m_5_s,d_val[gp==counter,],type="class")
xerrs_5[counter]<-sum(pred_5!=d_val$num[gp==counter])/sum(gp==counter)
}
#Time to check the means
mean(xerrs_f) #--> 0.4442688
mean(xerrs_d) #--> 0.4357708
mean(xerrs_5) #--> 0.3693676 So this new model beats both m_d and m_f
bagres <- bagging(num ~ .,
data = d_val,
nbagg = 500,
control = rpart.control(cp = 0, minsplit = 1, xval = 0),
aggregation = "majority")
predictions <- predict(bagres, newdata = d_val, type="class")
misclass_error <- mean(predictions != d_val$num)
print(misclass_error)#--> 0.9162996
b_pred <- predict(bagres, newdata = d_val, type = "class", aggregation = "majority")
b_pred <- predict(bagres, newdata = d_val, type = "class", aggregation = "majority")
sum(b_pred!=d_val$num)/dim(d_val)[1] #The same value --> 0.9162996
xerrs_b<-NA*numeric(numgp)
for (counter in 1:numgp)
{
#fit the models on all of the data excluding one group
bagres_s<-bagging(num~., #again, the ipred:: prefix is just to make sure we are using the bagging routine from the ipred package
data=d_val[gp!=counter,],nbagg=500,coob=FALSE,method="class",
na.action = na.omit,
control=rpart.control(cp=0, minsplit=1,xval=0),aggregation="majority")
predictions <- predict(bagres_s, newdata = d_val)
misclass_error <- mean(predictions != d_val$num)
print(misclass_error)
#get predictions for the left out group and get error rates
pred_b_s<-predict(bagres_s,d_val[gp==counter,],type="class")#,aggregation="majority")
xerrs_b[counter]<-sum(pred_b_s!=d_val$num[gp==counter])/sum(gp==counter)
}
mean(xerrs_f)#--> 0.4442688
mean(xerrs_d)#--> 0.4357708
mean(xerrs_5)#--> 0.3693676 *best value
mean(xerrs_b)#--> 1
m_rf<-randomForest(num~.,data=d_val,ntree=1000)
m_rf #includes oob error rate--> 37.44%
names(m_rf)
tail(m_rf$err.rate) #last entry of column 1 is the same as the oob error rate above
plot(m_rf) #You can also see as the number of trees increases the accuracy increases and then
#levels off
dim(m_rf$err.rate)[1] #corresponds to the number of trees=1000 trees
rf_pred<-predict(m_rf)
sum(m_rf$predicted==rf_pred)
length(rf_pred) #ok so predict is just pulling a slot that already is in m_rf
sum(rf_pred!=d_val$num)/dim(d_val)[1] #agrees with the oob error rate above --> 0.3744493
xerrs_rf<-NA*numeric(numgp)
for (counter in 1:numgp)
{
#fit the models on all of the data excluding one group
rfres_s<-randomForest(num~.,
data=d_val[gp!=counter,],ntree=1000)
#get predictions for the left out group and get error rates
pred_rf_s<-predict(rfres_s,d_val[gp==counter,],type="class")
xerrs_rf[counter]<-sum(pred_rf_s!=d_val$num[gp==counter])/sum(gp==counter)
}
mean(xerrs_f)#--> 0.4442688
mean(xerrs_d)#--> 0.4357708
mean(xerrs_5)#--> 0.3693676 *best value
mean(xerrs_b)#--> 1
mean(xerrs_rf)# --> 1
mean(xerrs_rf)# --> 0.3871542
m_ada <-boosting(num~.,
data = d_val,
mfinal = 10,
control = rpart.control(cp = 0, minsplit = 1, xval = 0))
d_val$age
ada_pred <- predict(m_ada, newdata = d_val)$class
sum(ada_pred!=d_val$num)/dim(d_val)[1]
m_ada$importance #gives the relative importance of the different variables for
xerrs_ada<-NA*numeric(numgp)
for (counter in 1:numgp)
{
print(paste(counter,"of",numgp))
#fit the models on all of the data excluding one group
adares_s<-boosting(num~.,data=d_val[gp!=counter,])
#get predictions for the left out group and get error rates
pred_ada_s<-predict(adares_s,d_val[gp==counter,],type="class")$class
xerrs_ada[counter]<-sum(pred_ada_s!=d_val$num[gp==counter])/sum(gp==counter)
}
mean(xerrs_f)#--> 0.4442688
mean(xerrs_d)#--> 0.4357708
mean(xerrs_5)#--> 0.3693676 *best value
mean(xerrs_b)#--> 1
mean(xerrs_rf)# --> 1
mean(xerrs_ada) #--> NA
#put the data in the specific form expected by the xgb function
X_val<-as.matrix(d_val[,2:(dim(d_val)[2])])
y_val<-as.integer(d_val[,1])-1
sapply(d_val, class)
d_val[] <- lapply(d_val, function(x) {
if (is.character(x)) as.numeric(x) else x
})
X_val <- as.matrix(d_val[, 2:(dim(d_val)[2])])  # Usa todas las columnas excepto 'num'
y_val <- as.integer(d_val[, 1]) - 1              # Asegúrate de que 'num' sea el índice correcto
class(X_val)
d_val$ca <- as.numeric(as.character(d_val$ca))
d_val$thal <- as.numeric(as.character(d_val$thal))
X_val <- as.matrix(d_val[, 2:(dim(d_val)[2])])  # Asegúrate de que ahora todo sea numérico
y_val <- as.integer(d_val[, 1]) - 1              # Asegúrate de que 'num' sea el índice correcto
sapply(X_val, class)  # Debería mostrar "numeric" para todas las columnas
sapply(d_val, class)
d_val[] <- lapply(d_val, function(x) {
if (is.factor(x)) {
as.numeric(as.character(x))  # Convierte factores a numéricos
} else if (is.character(x)) {
as.numeric(x)  # Convierte caracteres a numéricos
} else {
x  # Mantiene el tipo original si no es factor ni carácter
}
})
X_val <- as.matrix(d_val[, 2:(dim(d_val)[2])])  # Asegúrate de que ahora todo sea numérico
y_val <- as.integer(d_val[, 1]) - 1              # Asegúrate de que 'num' sea el índice correcto
sapply(X_val, class)  # Debería mostrar "numeric" para todas las columnas
y_val <- ifelse(d_val$num >= 2, 1, 0)  # 1 para "enfermo", 0 para "sano"
unique(y_val)  # Verifica los valores únicos en y_val
m_xgb<-xgboost(data=X_val,
label=y_val,
max_depth=6,eta=.3,
nthread=2,nrounds=20,
objective="binary:logistic",verbose=2)
#get error rate on the validation data
pred_xgb<-predict(m_xgb,X_val) #gives probabilities
#of being in the first class, which is additionally useful, tho
#we don't use it
predictions<-rep(0,length(pred_xgb))
predictions[pred_xgb>.5]<-"1"
#put the data in the specific form expected by the xgb function
X_val<-as.matrix(d_val[,2:(dim(d_val)[2])])
y_val<-as.integer(d_val[,1])-1
sapply(d_val, class)
d_val[] <- lapply(d_val, function(x) {
if (is.character(x)) as.numeric(x) else x
})
X_val <- as.matrix(d_val[, 2:(dim(d_val)[2])])
y_val <- as.integer(d_val[, 1]) - 1
class(X_val)
d_val$ca <- as.numeric(as.character(d_val$ca))
d_val$thal <- as.numeric(as.character(d_val$thal))
X_val <- as.matrix(d_val[, 2:(dim(d_val)[2])])
y_val <- as.integer(d_val[, 1]) - 1
sapply(X_val, class)
d_val[] <- lapply(d_val, function(x) {
if (is.factor(x)) {
as.numeric(as.character(x))
} else if (is.character(x)) {
as.numeric(x)
} else {
x
}
})
X_val <- as.matrix(d_val[, 2:(dim(d_val)[2])])
y_val <- as.integer(d_val[, 1]) - 1
sapply(X_val, class)
y_val <- ifelse(d_val$num >= 2, 1, 0)
unique(y_val)
m_xgb<-xgboost(data=X_val,
label=y_val,
max_depth=6,eta=.3,
nthread=2,nrounds=20,
objective="binary:logistic",verbose=2)
#get error rate on the validation data
pred_xgb<-predict(m_xgb,X_val) #gives probabilities
#of being in the first class, which is additionally useful, tho
#we don't use it
predictions<-rep(0,length(pred_xgb))
predictions[pred_xgb>.5]<-"1"
sum(predictions!=as.character(d_val$num))/dim(d_val)[1] #--> 0.4229075
xerrs_xgb<-NA*numeric(numgp)
for (counter in 1:numgp)
{
#status
print(paste(counter,"of",numgp))
#fit the models on all of the data excluding one group
m_xgb_s<-xgboost(data=X_val[gp!=counter,],
label=y_val[gp!=counter],
max_depth=7,eta=.2,subsample=0.5,nrounds=55,
nthread=2,objective="binary:logistic",verbose=0)
#get predictions for the left out group and get error rates
pred_xgb_s<-predict(m_xgb_s,X_val[gp==counter,])
predictions_s<-rep(0,length(pred_xgb_s))
predictions_s[pred_xgb_s>.5]<-"1"
xerrs_xgb[counter]<-sum(predictions_s!=
d_val$diagnosis[gp==counter])/sum(gp==counter)
}
mean(xerrs_f)#--> 0.4442688
mean(xerrs_d)#--> 0.4357708
mean(xerrs_5)#--> 0.3693676 *best value
mean(xerrs_b)#--> 1
mean(xerrs_rf)# --> 1
mean(xerrs_ada) #--> NA
mean(xerrs_xgb)#--> 0
importance_matrix<-xgb.importance(model=m_xgb)
importance_matrix
#**Now apply THAT ONE MODEL ONLY on the test data and see prediction error
#--Pruned CART
testpred_ada<-predict(m_f_5,d_test[,1:14],type="class")#$class
sum(testpred_ada!=d_test$num)/dim(d_test)[1] #--> 0.5526316
data <- data.frame(
Models = c("Prunned model", "Full CART", "Pruned CART", "Bagging Model",
"Random Forest Model", "Adaptive boosting",
"Extreme Gradient Boosting"),
"x-val accuracy" = c(0.4442688, 0.4357708, 0.3693676, 1, 0.3871542, 0.3828063, 0)
)
tabla_grafico <- tableGrob(data)
png("tabla.png", width = 600, height = 400)  # Ajusta el tamaño según sea necesario
grid.draw(tabla_grafico)
dev.off()
#**Now apply THAT ONE MODEL ONLY on the test data and see prediction error
#--Pruned CART
testpred_ada<-predict(m_f_5,d_test[,1:14],type="class")#$class
sum(testpred_ada!=d_test$num)/dim(d_test)[1] #--> 0.5263158
#This is the main code.
#Setting my working directory:
setwd("C:/Users/asque/Documents/ML/QuezadaMLbiol/UnitCARET)
#This is the main code.
#Setting my working directory:
setwd("C:/Users/asque/Documents/ML/QuezadaMLbiol/UnitCARET")
#This is the main code.
#Setting my working directory:
setwd("C:/Users/asque/Documents/ML/QuezadaMLbiol/UnitCARET")
data <- read.csv(file ="AirQualityUCI")
data <- read.csv(file ="UnitCARET/AirQualityUCI")
data <- read.csv(file ="UnitCARET/AirQualityUCI.csv")
data <- read.csv("C:/Users/asque/Documents/ML/QuezadaMLbiol/UnitCARET/AirQualityUCI.csv")
data <- read.csv("AirQualityUCI.csv", sep =";")
data
rm(list=ls())
#This is the main code.
#Setting my working directory:
setwd("C:/Users/asque/Documents/ML/QuezadaMLbiol/UnitCARET")
data <- read.csv("AirQualityUCI.csv", sep =";")
graphics.off()
library(caret)
#Data preparation
head(data)
dim(data)
set.seed(101)
inds<-createDataPartition(data$CO.GT,p=0.8,list=FALSE)
inds<-createDataPartition(data$PT08.S1.CO.,p=0.8,list=FALSE)
is.na(data)
data <- data[1:15]
data
is.na(data)
inds<-createDataPartition(data$CO.GT.,p=0.8,list=FALSE)
inds<-createDataPartition(data$PT08.S1.CO.,p=0.8,list=FALSE)
inds<-createDataPartition(data$NMHC.GT.,p=0.8,list=FALSE)
inds<-createDataPartition(data$NMHC.GT.,p=0.8,list=FALSE, na.rm=FALSE)
inds<-createDataPartition(data$NMHC.GT.,p=0.8,list=FALSE, na.rm=TRUE)
inds<-createDataPartition(data$C6H6.GT.,p=0.8,list=FALSE#, na.rm=TRUE)
inds
#C6H6 (GT) <- True hourly averaged Benzene concentration in microg/m^3 (reference analyzer)
#             microg/m^3
inds<-createDataPartition(data$C6H6.GT.,p=0.8,list=FALSE#, na.rm=TRUE)
inds
#C6H6 (GT) <- True hourly averaged Benzene concentration in microg/m^3 (reference analyzer)
#             microg/m^3
inds<-createDataPartition(data$C6H6.GT.,p=0.8,list=FALSE)
inds
#C6H6 (GT) <- True hourly averaged Benzene concentration in microg/m^3 (reference analyzer)
#             microg/m^3
inds<-createDataPartition(data$C6H6.GT.,p=0.8,list=FALSE)
#C6H6 (GT) <- True hourly averaged Benzene concentration in microg/m^3 (reference analyzer)
#             microg/m^3
inds<-createDataPartition(data$CO.GT.,p=0.8,list=FALSE)
#C6H6 (GT) <- True hourly averaged Benzene concentration in microg/m^3 (reference analyzer)
#             microg/m^3
inds<-createDataPartition(data$C6H6.GT.p=0.8,list=FALSE)
inds
#C6H6 (GT) <- True hourly averaged Benzene concentration in microg/m^3 (reference analyzer)
#             microg/m^3
inds<-createDataPartition(data$C6H6.GT.p=0.8,list=FALSE)
#C6H6 (GT) <- True hourly averaged Benzene concentration in microg/m^3 (reference analyzer)
#             microg/m^3
inds<-createDataPartition(data$C6H6.GT.,p=0.8,list=FALSE)
#C6H6 (GT) <- True hourly averaged Benzene concentration in microg/m^3 (reference analyzer)
#             microg/m^3
inds<-createDataPartition(data$CO.GT.,p=0.8,list=FALSE)
#C6H6 (GT) <- True hourly averaged Benzene concentration in microg/m^3 (reference analyzer)
#             microg/m^3
inds<-createDataPartition(data$C6H6.GT.,p=0.8,list=FALSE)
data
data <- read.csv("AirQualityUCI1.csv", sep =";")
data
data <- read.csv("AirQualityUCI1.csv")
data
#Data preparation
head(data)
dim(data)
set.seed(101)
data <- data[1:15]
is.na(data)
#I have some missing values are tagged with -200, then I'm going to impute these.
data[data == -200] <- NA
data1 <- preProcess(x = data,method = "knnImpute", k = 5 )
imputed_data <- predict(data1, data)
install.packages("RANN")
library(RANN)
imputed_data <- predict(data1, data)
data1
imputed_data <- predict(data1, data)
imputed_data
#I have some missing values are tagged with -200, then I'm going to impute these.
data_filtered <- data[rowSums(is.na(data)) != ncol(data), ]
data_filtered
#data[data == -200] <- NA
data1 <- preProcess(x = data,method = "knnImpute", k = 5 )
imputed_data <- predict(data1, data_filtered)
data_filtered[data_filtered == -200] <- NA
data1 <- preProcess(x = data_filtered,method = "knnImpute", k = 5 )
data1
imputed_data <- predict(data1, data_filtered)
#I have some missing values are tagged with -200, then I'm going to impute these.
data_filtered <- data[rowSums(is.na(data)) != ncol(data), ]
data_filtered
data_filtered[data_filtered == -200] <- NA
data1 <- preProcess(x = data_filtered,method = "knnImpute", k = 5 )
data1
imputed_data <- predict(data1, data_filtered)
row_missing_counts <- rowSums(is.na(data))
summary(row_missing_counts)
data_partial <- data[rowSums(is.na(data)) < ncol(data), ]
preProcess_model <- preProcess(data_partial, method = "knnImpute", k = 5)
imputed_data <- predict(preProcess_model, data_partial)
all_missing_rows <- rowSums(is.na(data_filtered)) == ncol(data_filtered)
sum(all_missing_rows)
data_filtered <- data_filtered[!all_missing_rows, ]
data_filtered
data1 <- preProcess(x = data_filtered,method = "knnImpute", k = 5 )
imputed_data <- predict(data1, data_filtered)
all_missing_rows <- rowSums(is.na(data_partial)) == ncol(data_partial)
all_missing_rows
sum(all_missing_rows)
col_missing_counts <- colSums(is.na(data_partial))
col_missing_counts
col_missing_counts[col_missing_counts > 0]
str(data_partial)
numeric_data <- data_partial[sapply(data_partial, is.numeric)]
preProcess_model <- preProcess(numeric_data, method = "knnImpute", k = 5)
imputed_data <- predict(preProcess_model, numeric_data)
#
library(doParallel)
cl <- makePSOCKcluster(5)
registerDoParallel(cl)
rm(list=ls())
graphics.off()
library(caret)
d<-read.csv(file="C:/Users/asque/Documents/ML/QuezadaMLbiol/UnitCART/breast_cancer/data.csv") #assumes the file stored in the working directory
head(d)
dim(d)
d<-d[,2:12] #keep only the "mean" columns
d$diagnosis<-as.factor(d$diagnosis)
names(d)<-c("diagnosis","radius","texture","perimeter","area","smoothness",
"compactness","concavity","concave_points","symmetry","fractal_dimension")
set.seed(101)
inds<-createDataPartition(d$diagnosis,p=0.8,list=FALSE)
d_val<-d[inds,]
d_test<-d[-inds,]
x_val<-d_val[,2:11]
y_val<-d_val[,1]
library(doParallel)
cl <- makePSOCKcluster(5) #the argument is basically the number of cores/processes to use
registerDoParallel(cl)
modelLookup("rpart") #Gives information on the model, the tuning parameters used by caret,
tc<-trainControl(method="repeatedcv",number=10,repeats=3) #makes it so that we will use
#k-fold cross validation with 10 folds, repeated 3 times
m_rpart<-train(x=x_val,y=y_val,method="rpart",trControl=tc)
class(m_rpart) #you get a custom object
names(m_rpart) #it has a lot of slots
tc<-trainControl(method="repeatedcv",number=10,repeats=3) #makes it so that we will use
#k-fold cross validation with 10 folds, repeated 3 times
m_rpart<-train(x=x_val,y=y_val,method="rpart",trControl=tc)
class(m_rpart) #you get a custom object
names(m_rpart) #it has a lot of slots
m_rpart #shows a pretty decent and clear summary of what it did and what it got
m_rpart$results #Shows that (by default) the routine only varied cp, and it
pred<-predict(m_rpart,d_val)
table(pred,y_val)
predfm<-predict(m_rpart$finalModel,d_val)
predfmc<-apply(X=predfm,MARGIN=1,FUN=function(x){ifelse(x[1]<.5,"M","B")})
sum(pred==predfmc) #shows the predictions from calling predict on m
#to do more values of cp using tuneLength
m_rpart<-train(x=x_val,y=y_val,method="rpart",trControl=tc,tuneLength=15)
m_rpart$results
plot(m_rpart$results$cp,1-m_rpart$results$Accuracy,type="l")
#control the specific values of cp to use using tuneGrid
tg<-data.frame(cp=seq(from=0.01,to=0.8,by=0.01))
m_rpart<-train(x=x_val,y=y_val,method="rpart",trControl=tc,tuneGrid=tg)
m_rpart$results
plot(m_rpart$results$cp,1-m_rpart$results$Accuracy,type="l")
max(m_rpart$results$Accuracy) #this is the best accuracy we got
#now do it with preprocessing (center and scale) - experimental
set.seed(101)
m_rpart1<-train(x=x_val,y=y_val,method="rpart",trControl=tc,tuneLength=15)
set.seed(101)
m_rpart2<-train(x=x_val,y=y_val,method="rpart",trControl=tc,tuneLength=15,preProcess=c("center","scale"))
cbind(m_rpart1$results$Accuracy,m_rpart2$results$Accuracy)
m_rpart1$results$Accuracy-m_rpart2$results$Accuracy #same results, as expected, since
modelLookup("knn")
m_knn<-train(x=x_val,y=y_val,method="knn",
trControl=tc,tuneLength=15,preProcess=c("center","scale"))
m_knn$results #k is the single metaparameter the routines varies
max(m_knn$results$Accuracy)
modelLookup("C5.0")
m_C5p0<-train(x=x_val,y=y_val,method="C5.0",
trControl=tc,tuneLength=3,preProcess=c("center","scale")) #tuneLength set to 3, here, because there are
m_C5p0<-train(x=x_val,y=y_val,method="C5.0",
trControl=tc,tuneLength=3,preProcess=c("center","scale")) #tuneLength set to 3, here, because there are
#three metaparameters for this model
m_C5p0$results #Some warnings to investigate if this turns out to be a
#useful method, also understand the search grid
max(m_C5p0$results$Accuracy)
tc<-trainControl(method="repeatedcv",number=10,repeats=3)
modelLookup("treebag")
modelLookup("rf")
modelLookup("AdaBoost.M1")
modelLookup("gbm")
modelLookup("LogitBoost")
modelLookup("svmLinear")
modelLookup("svmRadial")
modelLookup("nnet")
modelnames<-c("rpart","knn","C5.0", #what we did already, above
"treebag", #bagged CART, no tuning parameters
"rf", #random forest, one tuning parameter based on numbers of randomly selected predictors
"AdaBoost.M1", #adaptive boosting, three tuning parameters which control the tree used
"gbm", #gradient boosting machine, 4 tuning parameters
"LogitBoost", #Boosted logistic regression, one tuning parameter
"svmLinear", #Support vector machines with linear kernel, one tuning parameter
"svmRadial", #Support vector machines with radial kernel, two tuning parameters
"nnet") #neural net, two tuning parameters
tL<-c(15,15,3, #the values we used above for the first three models
1, #no tuning parameters for treebag, so does not matter
15, #rf
3, #AdaBoost.M1
5, #gbm
15, #LogitBoost
15, #svmLinear
5, #svmRadial
5) #nnet
modres<-list() #receptacle for saving all results
acc<-data.frame(model=modelnames,
Accuracy=NA*numeric(length(modelnames)),
Kappa=NA*numeric(length(modelnames)),
AccuracySD=NA*numeric(length(modelnames)),
KappaSD=NA*numeric(length(modelnames)))
for (counter in 1:length(modelnames))
{
print(paste0(modelnames[counter],"; ",counter," of ",length(modelnames)))
modres[[counter]]<-train(x=x_val,y=y_val,method=modelnames[counter],
trControl=tc,tuneLength = tL[counter],preProcess=c("center","scale"))
ind<-which(modres[[counter]]$results$Accuracy==max(modres[[counter]]$results$Accuracy))
ind<-ind[1]
acc[counter,2:5]<-modres[[counter]]$results[ind,c("Accuracy","Kappa","AccuracySD","KappaSD")]
}
acc
save.image(file="CaretClassificationDemo_WorkspaceAtEnd.RData")
stopCluster(cl)
rm(list=ls())
#This is the main code.
#Setting my working directory:
setwd("C:/Users/asque/Documents/ML/QuezadaMLbiol/UnitCARET")
data <- read.csv("AirQualityUCI1.csv")
graphics.off()
library(caret)
library(RANN)
#Data preparation
head(data)
dim(data)
set.seed(101)
data <- data[1:15]
is.na(data)
#C6H6 (GT) <- True hourly averaged Benzene concentration in microg/m^3 (reference analyzer)
#             microg/m^3
inds<-createDataPartition(data$C6H6.GT.,p=0.8,list=FALSE)
#C6H6 (GT) <- True hourly averaged Benzene concentration in microg/m^3 (reference analyzer)
#             microg/m^3
inds<-createDataPartition(data$C6H6.GT.,p=0.8,list=FALSE)
is.na(data)
#C6H6 (GT) <- True hourly averaged Benzene concentration in microg/m^3 (reference analyzer)
#             microg/m^3
inds<-createDataPartition(data$C6H6.GT.,p=0.8,list=FALSE)
heart <- read.table("processed.cleveland.data", header = F, sep = ",")
#This is the main code.
#Setting my working directory:
setwd("C:/Users/asque/Documents/ML/QuezadaMLbiol/UnitCART/heart_disease/heart+disease")
heart <- read.table("processed.cleveland.data", header = F, sep = ",")
head(heart)
names(heart)<-c("age", "sex", "cp", "trestbps", "chol", "fbs", "restecg",
"thalach", "exang", "oldpeak", "slope", "ca", "thal", "num")
dim(heart)
library(caret)
library(RANN)
set.seed(101)
data <- heart[1:15]
heart
#data <- heart[1:15]
is.na(heart)
#C6H6 (GT) <- True hourly averaged Benzene concentration in microg/m^3 (reference analyzer)
#             microg/m^3
inds<-createDataPartition(heart$cp,p=0.8,list=FALSE)
inds
