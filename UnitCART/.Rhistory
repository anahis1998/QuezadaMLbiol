# install usethis package (if needed)
install.packages("usethis")
# step 1 (create the project: this creates a new folder)
usethis::create_project("path/to/new_directory")  # usually opens a new Rstudio
# step 2 (connect it with git; answer the questions in the console)
usethis::use_git()  # commit changes; restart RStudio
# step 1 (create the project: this creates a new folder)
usethis::create_project("C:/Users/asque/Documents/git_workshop")  # usually opens a new Rstudio
# step 2 (connect it with git; answer the questions in the console)
usethis::use_git()  # commit changes; restart RStudio
# step 1 (create the project: this creates a new folder)
usethis::create_project("C:/Users/asque/Documents/new_github_seminar")  # usually opens a new Rstudio
# step 1 (create the project: this creates a new folder)
usethis::create_project("C:/Users/asque/Documents/week_6")  # usually opens a new Rstudio
library(caret)
d<-read.csv(file="C:/Users/asque/Documents/ML/QuezadaMLbiol/UnitCART/breast_cancer") #assumes the file stored in the working directory
#**load the data and simplify, same as I did in the CART unit
setwd("C:/Users/asque/Documents/ML/QuezadaMLbiol/UnitCART")
d<-read.csv(file="C:/Users/asque/Documents/ML/QuezadaMLbiol/UnitCART/breast_cancer") #assumes the file stored in the working directory
d<-read.csv(file="C:/Users/asque/Documents/ML/QuezadaMLbiol/UnitCART/breast_cancer.csv") #assumes the file stored in the working directory
#**load the data and simplify, same as I did in the CART unit
setwd("C:/Users/asque/Documents/ML/QuezadaMLbiol/UnitCART")
d<-read.csv(file="C:/Users/asque/Documents/ML/QuezadaMLbiol/UnitCART/breast_cancer/BreastCancerData.csv") #assumes the file stored in the working directory
head(d)
dim(d)
d<-d[,2:12] #keep only the "mean" columns
d$diagnosis<-as.factor(d$diagnosis)
names(d)<-c("diagnosis","radius","texture","perimeter","area","smoothness",
"compactness","concavity","concave_points","symmetry","fractal_dimension")
set.seed(101)
inds<-createDataPartition(d$diagnosis,p=0.8,list=FALSE)
inds
d_val<-d[inds,]
d_val
d_test<-d[-inds,]
d_test
x_val
x_val<-d_val[,2:11]
x_val
cl <- makePSOCKcluster(5) #the argument is basically the number of cores/processes to use
library(doParallel)
cl <- makePSOCKcluster(5) #the argument is basically the number of cores/processes to use
cl
cl <- makePSOCKcluster(6) #the argument is basically the number of cores/processes to use
cl
modelLookup("rpart") #Gives information on the model, the tuning parameters used by caret,
tc<-trainControl(method="repeatedcv",number=10,repeats=3) #makes it so that we will use
tc
#k-fold cross validation with 10 folds, repeated 3 times
m_rpart<-train(x=x_val,y=y_val,method="rpart",trControl=tc)
y_val<-d_val[,1]
#k-fold cross validation with 10 folds, repeated 3 times
m_rpart<-train(x=x_val,y=y_val,method="rpart",trControl=tc)
head
head(d)
d<-d[,2:12] #keep only the "mean" columns
d<-d[,2:12] #keep only the "mean" columns
d<-read.csv(file="C:/Users/asque/Documents/ML/QuezadaMLbiol/UnitCART/breast_cancer/BreastCancerData.csv") #assumes the file stored in the working directory
head(d)
x_val<-d_val[,2:11]
x_val
y_val<-d_val[,1]
y_val
d_val<-d[inds,]
d_val
head(d)
y_val<-d_val[,1]
y_val
inds<-createDataPartition(d$diagnosis,p=0.8,list=FALSE)
d_val<-d[inds,]
d_val
d<-d[,2:12] #keep only the "mean" columns
d
x_val<-d_val[,2:11]
x_val
head(d)
head(d_val)
head(x_val)
head(y_val)
library(doParallel)
cl <- makePSOCKcluster(6) #the argument is basically the number of cores/processes to use
registerDoParallel(cl)
registerDoParallel(cl)
modelLookup("rpart") #Gives information on the model, the tuning parameters used by caret,
tc<-trainControl(method="repeatedcv",number=10,repeats=3) #makes it so that we will use
#k-fold cross validation with 10 folds, repeated 3 times
m_rpart<-train(x=x_val,y=y_val,method="rpart",trControl=tc)
m_rpart
class(m_rpart) #you get a custom object
names(m_rpart) #it has a lot of slots
m_rpart #shows a pretty decent and clear summary of what it did and what it got
m_rpart$results #Shows that (by default) the routine only varied cp, and it
pred<-predict(m_rpart,d_val)
table(pred,y_val)
predfm<-predict(m_rpart$finalModel,d_val)
predfm
predfmc<-apply(X=predfm,MARGIN=1,FUN=function(x){ifelse(x[1]<.5,"M","B")})
predfm<-predict(m_rpart$finalModel,d_val)
predfmc<-apply(X=predfm,MARGIN=1,FUN=function(x){ifelse(x[1]<.5,"M","B")})
pred<-predict(m_rpart,d_val)
table(pred,y_val)
predfm<-predict(m_rpart$finalModel,d_val)
predfmc<-apply(X=predfm,MARGIN=1,FUN=function(x){ifelse(x[1]<.5,"M","B")})
sum(pred==predfmc) #shows the predictions from calling predict on m
predfmc<-apply(X=predfm,MARGIN=1,FUN=function(x){ifelse(x[1]<.5,"M","B")})
predfm
predfmc
predfmc<-apply(X=predfm,MARGIN=1,FUN=function(x){ifelse(x[1]<.5,"M","B")})
table(pred,y_val)
predfm<-predict(m_rpart$finalModel,d_val)
predfm
pred==predfmc
#to do more values of cp using tuneLength
m_rpart<-train(x=x_val,y=y_val,method="rpart",trControl=tc,tuneLength=15)
#to do more values of cp using tuneLength
m_rpart<-train(x=x_val,y=y_val,method="rpart",trControl=tc,tuneLength=10)
#to do more values of cp using tuneLength
m_rpart<-train(x=x_val,y=y_val,method="rpart",trControl=tc,tuneLength=5)
#to do more values of cp using tuneLength
m_rpart<-train(x=x_val,y=y_val,method="rpart",trControl=tc,tuneLength=1)
#to do more values of cp using tuneLength
m_rpart<-train(x=x_val,y=y_val,method="rpart",trControl=tc,tuneLength=15)
modelLookup("C5.0")
pred<-predict(m_rpart,d_val)
table(pred,y_val)
predfm<-predict(m_rpart$finalModel,d_val)
predfmc<-apply(X=predfm,MARGIN=1,FUN=function(x){ifelse(x[1]<.5,"M","B")})
sum(pred==predfmc) #shows the predictions from calling predict on m
#to do more values of cp using tuneLength
m_rpart<-train(x=x_val,y=y_val,method="rpart",trControl=tc,tuneLength=15)
m_rpart$results
plot(m_rpart$results$cp,1-m_rpart$results$Accuracy,type="l")
#control the specific values of cp to use using tuneGrid
tg<-data.frame(cp=seq(from=0.01,to=0.8,by=0.01))
m_rpart<-train(x=x_val,y=y_val,method="rpart",trControl=tc,tuneGrid=tg)
m_rpart$results
plot(m_rpart$results$cp,1-m_rpart$results$Accuracy,type="l")
max(m_rpart$results$Accuracy) #this is the best accuracy we got
#now do it with preprocessing (center and scale) - experimental
set.seed(101)
m_rpart1<-train(x=x_val,y=y_val,method="rpart",trControl=tc,tuneLength=15)
set.seed(101)
m_rpart2<-train(x=x_val,y=y_val,method="rpart",trControl=tc,tuneLength=15,preProcess=c("center","scale"))
cbind(m_rpart1$results$Accuracy,m_rpart2$results$Accuracy)
m_rpart1$results$Accuracy-m_rpart2$results$Accuracy #same results, as expected, since
modelLookup("knn")
m_knn<-train(x=x_val,y=y_val,method="knn",
trControl=tc,tuneLength=15,preProcess=c("center","scale"))
m_knn$results #k is the single metaparameter the routines varies
m_knn<-train(x=x_val,y=y_val,method="knn",
trControl=tc,tuneLength=15,preProcess=c("center","scale"))
m_knn$results #k is the single metaparameter the routines varies
max(m_knn$results$Accuracy)
modelLookup("C5.0")
m_C5p0<-train(x=x_val,y=y_val,method="C5.0",
trControl=tc,tuneLength=3,preProcess=c("center","scale")) #tuneLength set to 3, here, because there are
#three metaparameters for this model
m_C5p0$results #Some warnings to investigate if this turns out to be a
#useful method, also understand the search grid
max(m_C5p0$results$Accuracy)
tc<-trainControl(method="repeatedcv",number=10,repeats=3)
modelLookup("treebag")
modelLookup("rf")
modelLookup("AdaBoost.M1")
modelLookup("gbm")
modelLookup("LogitBoost")
modelLookup("svmLinear")
modelLookup("svmRadial")
modelLookup("nnet")
modelnames<-c("rpart","knn","C5.0", #what we did already, above
"treebag", #bagged CART, no tuning parameters
"rf", #random forest, one tuning parameter based on numbers of randomly selected predictors
"AdaBoost.M1", #adaptive boosting, three tuning parameters which control the tree used
"gbm", #gradient boosting machine, 4 tuning parameters
"LogitBoost", #Boosted logistic regression, one tuning parameter
"svmLinear", #Support vector machines with linear kernel, one tuning parameter
"svmRadial", #Support vector machines with radial kernel, two tuning parameters
"nnet") #neural net, two tuning parameters
tL<-c(15,15,3, #the values we used above for the first three models
1, #no tuning parameters for treebag, so does not matter
15, #rf
3, #AdaBoost.M1
5, #gbm
15, #LogitBoost
15, #svmLinear
5, #svmRadial
5) #nnet
modres<-list() #receptacle for saving all results
acc<-data.frame(model=modelnames,
Accuracy=NA*numeric(length(modelnames)),
Kappa=NA*numeric(length(modelnames)),
AccuracySD=NA*numeric(length(modelnames)),
KappaSD=NA*numeric(length(modelnames)))
for (counter in 1:length(modelnames))
{
print(paste0(modelnames[counter],"; ",counter," of ",length(modelnames)))
modres[[counter]]<-train(x=x_val,y=y_val,method=modelnames[counter],
trControl=tc,tuneLength = tL[counter],preProcess=c("center","scale"))
ind<-which(modres[[counter]]$results$Accuracy==max(modres[[counter]]$results$Accuracy))
ind<-ind[1]
acc[counter,2:5]<-modres[[counter]]$results[ind,c("Accuracy","Kappa","AccuracySD","KappaSD")]
}
#**load the data and simplify, same as I did in the CART unit
setwd("C:/Users/asque/Documents/ML/QuezadaMLbiol/UnitCART")
d<-read.csv(file="C:/Users/asque/Documents/ML/QuezadaMLbiol/UnitCART/breast_cancer/BreastCancerData.csv") #assumes the file stored in the working directory
class(d)
head(d)
dim(d)
d<-d[,2:12] #keep only the "mean" columns
d$diagnosis<-as.factor(d$diagnosis)
names(d)<-c("diagnosis","radius","texture","perimeter","area","smoothness",
"compactness","concavity","concave_points","symmetry","fractal_dimension")
set.seed(101)
inds<-createDataPartition(d$diagnosis,p=0.8,list=FALSE)
d_val<-d[inds,]
d_test<-d[-inds,]
x_val<-d_val[,2:11]
y_val<-d_val[,1]
library(doParallel)
cl <- makePSOCKcluster(6) #the argument is basically the number of cores/processes to use
registerDoParallel(cl)
modelLookup("rpart") #Gives information on the model, the tuning parameters used by caret,
tc<-trainControl(method="repeatedcv",number=10,repeats=3) #makes it so that we will use
#k-fold cross validation with 10 folds, repeated 3 times
m_rpart<-train(x=x_val,y=y_val,method="rpart",trControl=tc)
class(m_rpart) #you get a custom object
names(m_rpart) #it has a lot of slots
m_rpart #shows a pretty decent and clear summary of what it did and what it got
m_rpart$results #Shows that (by default) the routine only varied cp, and it
pred<-predict(m_rpart,d_val)
table(pred,y_val)
predfm<-predict(m_rpart$finalModel,d_val)
predfmc<-apply(X=predfm,MARGIN=1,FUN=function(x){ifelse(x[1]<.5,"M","B")})
sum(pred==predfmc) #shows the predictions from calling predict on m
#to do more values of cp using tuneLength
m_rpart<-train(x=x_val,y=y_val,method="rpart",trControl=tc,tuneLength=15)
m_rpart$results
plot(m_rpart$results$cp,1-m_rpart$results$Accuracy,type="l")
#control the specific values of cp to use using tuneGrid
tg<-data.frame(cp=seq(from=0.01,to=0.8,by=0.01))
m_rpart<-train(x=x_val,y=y_val,method="rpart",trControl=tc,tuneGrid=tg)
m_rpart$results
plot(m_rpart$results$cp,1-m_rpart$results$Accuracy,type="l")
max(m_rpart$results$Accuracy) #this is the best accuracy we got
#now do it with preprocessing (center and scale) - experimental
set.seed(101)
m_rpart1<-train(x=x_val,y=y_val,method="rpart",trControl=tc,tuneLength=15)
set.seed(101)
m_rpart2<-train(x=x_val,y=y_val,method="rpart",trControl=tc,tuneLength=15,preProcess=c("center","scale"))
cbind(m_rpart1$results$Accuracy,m_rpart2$results$Accuracy)
m_rpart1$results$Accuracy-m_rpart2$results$Accuracy #same results, as expected, since
modelLookup("knn")
m_knn<-train(x=x_val,y=y_val,method="knn",
trControl=tc,tuneLength=15,preProcess=c("center","scale"))
m_knn$results #k is the single metaparameter the routines varies
max(m_knn$results$Accuracy)
modelLookup("C5.0")
m_C5p0<-train(x=x_val,y=y_val,method="C5.0",
trControl=tc,tuneLength=3,preProcess=c("center","scale")) #tuneLength set to 3, here, because there are
#three metaparameters for this model
m_C5p0$results #Some warnings to investigate if this turns out to be a
#useful method, also understand the search grid
max(m_C5p0$results$Accuracy)
tc<-trainControl(method="repeatedcv",number=10,repeats=3)
modelLookup("treebag")
modelLookup("rf")
modelLookup("AdaBoost.M1")
modelLookup("gbm")
modelLookup("LogitBoost")
modelLookup("svmLinear")
modelLookup("svmRadial")
modelLookup("nnet")
modelnames<-c("rpart","knn","C5.0", #what we did already, above
"treebag", #bagged CART, no tuning parameters
"rf", #random forest, one tuning parameter based on numbers of randomly selected predictors
"AdaBoost.M1", #adaptive boosting, three tuning parameters which control the tree used
"gbm", #gradient boosting machine, 4 tuning parameters
"LogitBoost", #Boosted logistic regression, one tuning parameter
"svmLinear", #Support vector machines with linear kernel, one tuning parameter
"svmRadial", #Support vector machines with radial kernel, two tuning parameters
"nnet") #neural net, two tuning parameters
tL<-c(15,15,3, #the values we used above for the first three models
1, #no tuning parameters for treebag, so does not matter
15, #rf
3, #AdaBoost.M1
5, #gbm
15, #LogitBoost
15, #svmLinear
5, #svmRadial
5) #nnet
modres<-list() #receptacle for saving all results
acc<-data.frame(model=modelnames,
Accuracy=NA*numeric(length(modelnames)),
Kappa=NA*numeric(length(modelnames)),
AccuracySD=NA*numeric(length(modelnames)),
KappaSD=NA*numeric(length(modelnames)))
for (counter in 1:length(modelnames))
{
print(paste0(modelnames[counter],"; ",counter," of ",length(modelnames)))
modres[[counter]]<-train(x=x_val,y=y_val,method=modelnames[counter],
trControl=tc,tuneLength = tL[counter],preProcess=c("center","scale"))
ind<-which(modres[[counter]]$results$Accuracy==max(modres[[counter]]$results$Accuracy))
ind<-ind[1]
acc[counter,2:5]<-modres[[counter]]$results[ind,c("Accuracy","Kappa","AccuracySD","KappaSD")]
}
acc
sapply(d, class)
sapply(inds, class)
d<-read.csv(file="C:/Users/asque/Documents/ML/QuezadaMLbiol/UnitCART/breast_cancer/BreastCancerData.csv") #assumes the file stored in the working directory
head(d)
dim(d)
d<-d[,2:12] #keep only the "mean" columns
d
d$diagnosis<-as.factor(d$diagnosis)
d$diagnosis
set.seed(101)
inds<-createDataPartition(d$diagnosis,p=0.8,list=FALSE)
d_val<-d[inds,]
d_test<-d[-inds,]
x_val<-d_val[,2:11]
y_val<-d_val[,1]
library(doParallel)
cl <- makePSOCKcluster(6) #the argument is basically the number of cores/processes to use
registerDoParallel(cl)
modelLookup("rpart") #Gives information on the model, the tuning parameters used by caret,
tc<-trainControl(method="repeatedcv",number=10,repeats=3) #makes it so that we will use
#k-fold cross validation with 10 folds, repeated 3 times
m_rpart<-train(x=x_val,y=y_val,method="rpart",trControl=tc)
class(m_rpart) #you get a custom object
names(m_rpart) #it has a lot of slots
m_rpart #shows a pretty decent and clear summary of what it did and what it got
m_rpart$results #Shows that (by default) the routine only varied cp, and it
pred<-predict(m_rpart,d_val)
table(pred,y_val)
predfm<-predict(m_rpart$finalModel,d_val)
predfmc<-apply(X=predfm,MARGIN=1,FUN=function(x){ifelse(x[1]<.5,"M","B")})
sum(pred==predfmc) #shows the predictions from calling predict on m
pred
predfmc
sum(pred==predfmc) #shows the predictions from calling predict on m
#to do more values of cp using tuneLength
m_rpart<-train(x=x_val,y=y_val,method="rpart",trControl=tc,tuneLength=15)
m_rpart$results
plot(m_rpart$results$cp,1-m_rpart$results$Accuracy,type="l")
#control the specific values of cp to use using tuneGrid
tg<-data.frame(cp=seq(from=0.01,to=0.8,by=0.01))
m_rpart<-train(x=x_val,y=y_val,method="rpart",trControl=tc,tuneGrid=tg)
m_rpart$results
plot(m_rpart$results$cp,1-m_rpart$results$Accuracy,type="l")
max(m_rpart$results$Accuracy) #this is the best accuracy we got
#now do it with preprocessing (center and scale) - experimental
set.seed(101)
m_rpart1<-train(x=x_val,y=y_val,method="rpart",trControl=tc,tuneLength=15)
set.seed(101)
m_rpart2<-train(x=x_val,y=y_val,method="rpart",trControl=tc,tuneLength=15,preProcess=c("center","scale"))
cbind(m_rpart1$results$Accuracy,m_rpart2$results$Accuracy)
m_rpart1$results$Accuracy-m_rpart2$results$Accuracy #same results, as expected, since
modelLookup("knn")
m_rpart1$results$Accuracy-m_rpart2$results$Accuracy #same results, as expected, since
modelLookup("knn")
m_knn<-train(x=x_val,y=y_val,method="knn",
trControl=tc,tuneLength=15,preProcess=c("center","scale"))
m_knn$results #k is the single metaparameter the routines varies
max(m_knn$results$Accuracy)
modelLookup("C5.0")
m_C5p0<-train(x=x_val,y=y_val,method="C5.0",
trControl=tc,tuneLength=3,preProcess=c("center","scale")) #tuneLength set to 3, here, because there are
#three metaparameters for this model
m_C5p0$results #Some warnings to investigate if this turns out to be a
#useful method, also understand the search grid
max(m_C5p0$results$Accuracy)
tc<-trainControl(method="repeatedcv",number=10,repeats=3)
modelLookup("treebag")
modelLookup("rf")
modelLookup("AdaBoost.M1")
modelLookup("gbm")
modelLookup("LogitBoost")
modelLookup("svmLinear")
modelLookup("svmRadial")
modelLookup("nnet")
modelnames<-c("rpart","knn","C5.0", #what we did already, above
"treebag", #bagged CART, no tuning parameters
"rf", #random forest, one tuning parameter based on numbers of randomly selected predictors
"AdaBoost.M1", #adaptive boosting, three tuning parameters which control the tree used
"gbm", #gradient boosting machine, 4 tuning parameters
"LogitBoost", #Boosted logistic regression, one tuning parameter
"svmLinear", #Support vector machines with linear kernel, one tuning parameter
"svmRadial", #Support vector machines with radial kernel, two tuning parameters
"nnet") #neural net, two tuning parameters
tL<-c(15,15,3, #the values we used above for the first three models
1, #no tuning parameters for treebag, so does not matter
15, #rf
3, #AdaBoost.M1
5, #gbm
15, #LogitBoost
15, #svmLinear
5, #svmRadial
5) #nnet
modres<-list() #receptacle for saving all results
acc<-data.frame(model=modelnames,
Accuracy=NA*numeric(length(modelnames)),
Kappa=NA*numeric(length(modelnames)),
AccuracySD=NA*numeric(length(modelnames)),
KappaSD=NA*numeric(length(modelnames)))
for (counter in 1:length(modelnames))
{
print(paste0(modelnames[counter],"; ",counter," of ",length(modelnames)))
modres[[counter]]<-train(x=x_val,y=y_val,method=modelnames[counter],
trControl=tc,tuneLength = tL[counter],preProcess=c("center","scale"))
ind<-which(modres[[counter]]$results$Accuracy==max(modres[[counter]]$results$Accuracy))
ind<-ind[1]
acc[counter,2:5]<-modres[[counter]]$results[ind,c("Accuracy","Kappa","AccuracySD","KappaSD")]
}
acc
inds<-createDataPartition(d$diagnosis,p=0.8,list=FALSE)
d_val<-d[inds,]
d_test<-d[-inds,]
d_val
d_test<-d[-inds,]
d_test
